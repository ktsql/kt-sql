# check list

## SQL测试

### 测试说明

KtSQL虽然采用MySQL协议作为其通信协议，但未全面兼容MySQL的全部协议命令，
测试的目的在于找出常见的协议哪些存在问题，并整理未能支持的协议列表

### 测试方法

- 使用jdbc调用mysql driver访问
- http://www.tpc.org/
- 使用第三方client访问

### 测试结果


## 性能测试

以下是从理论上对KtSQL各节点进行性能估算，并预估搭建集群需要的规模。

### 前端节点

KtSQL前端节点，单核的TPS为200，在8核的机器上，TPS达到1500，16核TPS达3000，
32核TPS达到6000，依据主流的服务器搭配，32核的服务器配128G内存，
32核128G内存的配置可实现千万级数据的内存计算，单机的TPS为5000左右。

参考salesforce 2013年的服务端数据，其峰值每秒数据库的tps达到2.4万，
以salesforce 2013年约30亿美金的营收规模估算其付费用户规模为300万，
考虑到有部分用户为试用型的用户，salesforce的用户规模可以估算为400万，
要支撑同样规模的用户数，KtSQL的前端节点，应该要达到10台以上的规模。

以中国互联网的双十一活动为例，2017年秒级交易创建峰值达到了32.5万，
这个业务规模是salesforce的10倍，这需要KtSQL的前端节点达到100台以上规模。

注：单核对应的内存为4G，即建议单核处理数据的规模<4G。简单计算时，单核处理记录条数达千万级（10G）

### HBase节点

以32核128G配置的HBase节点性能评估，单机节点可达到过万的TPS，如果要配合KtSQL前端10台节点，
估算TPS为5万，则HBase的节点规模建议为5台以上，同时要部署两台hbase master节点搭建集群。
考虑到单机节点存储建议不超过10T的存储量，集群支撑的数据规模上限在50T，集群支撑的最大存储数据规模可达百亿级。

此处假设集群支持的数据量规模为50T

考虑到HBase数据的读性能<写性能，读性能scan为最耗时的操作，在全读的情况下（假设数据有压缩，百万级数据<1G），
单节点可以每秒读取200M以上数据，5节点可满足1G的数据吞吐需求，完成百万级别数据规模的秒级读取操作，
假设需要读取的数据在整体数据量中占较小规模，则在响应式数据处理场景下，秒级完成1/10000整体规模的数据处理是合理的。

这里是网易云对HBase的性能测试报告
https://www.cnblogs.com/163yun/p/9661570.html

### Hadoop节点

hadoop节点用于为hbase提供持久化的存储，所需节点满足hbase的存储规模*3即可，按单机24T*6节点～50T*3配置

### Tephra节点

Tephra是单机节点，并通过zookeeper的支持，实现了3节点的高可用，所以搭建Tephra需要3台机器，
为保证Tephra的性能，可考虑采用高配的服务器，如64核256G内存

### zookeeper节点

Zookeeper为HBase和Tephra提供高可用服务，建议3台机器

### 百亿级数据量集群规模（按配置32核128G的级别配置）

- 前端节点10台服务器（ktsql stable version）
- 7台HBase（2.x）
- 6台Hadoop构建hdfs（2.7.x）
- 3台Tephra（0.15+）
- 3台zookeeper（Version 3.4.3 through 3.4.5）
- 2台haproxy用于前端节点负载均衡（stable）
